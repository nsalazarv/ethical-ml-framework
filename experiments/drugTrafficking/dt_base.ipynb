{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drug Traffiking experiment: base model\n",
    "\n",
    "This notebooks contains the data processing, building and training part of the model used for the base iteration of the drug traffiking experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import random\n",
    "import witwidget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import layers\n",
    "from tensorflow import keras\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from witwidget.notebook.visualization import WitWidget, WitConfigBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a tf feature spec from the dataframe and columns specified.\n",
    "def create_feature_spec(df, columns=None):\n",
    "    feature_spec = {}\n",
    "    if columns == None:\n",
    "        columns = df.columns.values.tolist()\n",
    "    for f in columns:\n",
    "        if df[f].dtype is np.dtype(np.int64):\n",
    "            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.int64)\n",
    "        elif df[f].dtype is np.dtype(np.float64):\n",
    "            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.float32)\n",
    "        else:\n",
    "            feature_spec[f] = tf.io.FixedLenFeature(shape=(), dtype=tf.string)\n",
    "    return feature_spec\n",
    "\n",
    "\n",
    "def create_feature_columns(columns, feature_spec):\n",
    "    ret = []\n",
    "    for col in columns:\n",
    "        if feature_spec[col].dtype is tf.int64 or feature_spec[col].dtype is tf.float32:\n",
    "            ret.append(tf.feature_column.numeric_column(col))\n",
    "        else:\n",
    "            ret.append(tf.feature_column.indicator_column(\n",
    "                tf.feature_column.categorical_column_with_vocabulary_list(col, list(df[col].unique()))))\n",
    "    return ret\n",
    "\n",
    "# An input function for providing input to a model from tf.Examples\n",
    "def tfexamples_input_fn(examples, feature_spec, label, mode=tf.estimator.ModeKeys.EVAL,\n",
    "                       num_epochs=None,\n",
    "                       batch_size=64):\n",
    "    def ex_generator():\n",
    "        for i in range(len(examples)):\n",
    "            yield examples[i].SerializeToString()\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "      ex_generator, tf.dtypes.string, tf.TensorShape([]))\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        dataset = dataset.shuffle(buffer_size=2 * batch_size + 1)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(lambda tf_example: parse_tf_example(tf_example, label, feature_spec))\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    return dataset\n",
    "\n",
    "# Parses Tf.Example protos into features for the input function.\n",
    "def parse_tf_example(example_proto, label, feature_spec):\n",
    "    parsed_features = tf.io.parse_example(serialized=example_proto, features=feature_spec)\n",
    "    target = parsed_features.pop(label)\n",
    "    return parsed_features, target\n",
    "\n",
    "# Converts a dataframe into a list of tf.Example protos.\n",
    "def df_to_examples(df, columns=None):\n",
    "    examples = []\n",
    "    if columns == None:\n",
    "        columns = df.columns.values.tolist()\n",
    "    for index, row in df.iterrows():\n",
    "        example = tf.train.Example()\n",
    "        for col in columns:\n",
    "            if df[col].dtype is np.dtype(np.int64):\n",
    "                example.features.feature[col].int64_list.value.append(int(row[col]))\n",
    "            elif df[col].dtype is np.dtype(np.float64):\n",
    "                example.features.feature[col].float_list.value.append(row[col])\n",
    "            elif row[col] == row[col]:\n",
    "                example.features.feature[col].bytes_list.value.append(row[col].encode('utf-8'))\n",
    "        examples.append(example)\n",
    "    return examples\n",
    "\n",
    "# Converts a dataframe column into a column of 0's and 1's based on the provided test.\n",
    "# Used to force label columns to be numeric for binary classification using a TF estimator.\n",
    "def make_label_column_numeric(df, label_column, test):\n",
    "  df[label_column] = np.where(test(df[label_column]), 1, 0)\n",
    "\n",
    "def minmax_scaler(data):\n",
    "  scaler = MinMaxScaler()\n",
    "  scaled = scaler.fit_transform(data)\n",
    "  return scaled\n",
    "\n",
    "def process_data(data):\n",
    "  x = data.loc[:, data.columns != 'Tipo salida 2']\n",
    "  y = data['Tipo salida 2']\n",
    "\n",
    "  x_cat = x[['Region', 'Defensor', 'Desarrollo','extranjero']]\n",
    "  x_cat['Region'] = label_encoder.fit_transform(x_cat['Region'])\n",
    "  x_cat['Defensor'] = label_encoder.fit_transform(x_cat['Defensor'])\n",
    "  x_cat['Desarrollo'] = label_encoder.fit_transform(x_cat['Desarrollo'])\n",
    "  x_cat['extranjero'] = label_encoder.fit_transform(x_cat['extranjero'])\n",
    "\n",
    "  x_num = x.loc[:, ~x.columns.isin(x_cat.columns)]\n",
    "\n",
    "  x_norm = minmax_scaler(x_num)\n",
    "  x_norm = pd.DataFrame(x_norm, columns = x_num.columns)\n",
    "\n",
    "  x_norm.reset_index(drop=True, inplace=True)\n",
    "  x_cat.reset_index(drop=True, inplace=True)\n",
    "\n",
    "  x_fin = pd.concat([x_norm, x_cat], axis = 1)\n",
    "  #y_fin = label_encoder.fit_transform(y)\n",
    "\n",
    "  return x_fin\n",
    "\n",
    "def custom_predict(examples_to_infer):\n",
    "\n",
    "  preds = model1.predict(model_inputs)\n",
    "  preds = [[1 - pred[0], pred[0]] for pred in preds]\n",
    "  return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'traficoDrogas.csv'\n",
    "\n",
    "data = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cambiando las dos RM a una sola\n",
    "\n",
    "data['Región (tribunal)']=data['Región (tribunal)'].replace('Metropolitana Sur','Metropolitana')\n",
    "data['Región (tribunal)']=data['Región (tribunal)'].replace('Metropolitana Norte','Metropolitana')\n",
    "\n",
    "## Sacar filas erróneas\n",
    "\n",
    "data.drop(16690, inplace=True)\n",
    "data.drop(16691, inplace=True)\n",
    "\n",
    "## Agregar variable edad\n",
    "\n",
    "data['Edad'] = np.nan\n",
    "\n",
    "mu = 27 ## Edad promedio entre 18 y 34 años (concentran la mayoría de consumo de drogas) (estudio drogas senda 2020)\n",
    "sigma = 8\n",
    "random.seed(23)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data['Edad'][i] = round(max(18, min(np.random.normal(mu, sigma), 65)))\n",
    "\n",
    "## Agregar variable extranjero\n",
    "\n",
    "data['Extranjero'] = np.nan\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if data['P.S. Expulsión'][i] == 'N':\n",
    "        data['Extranjero'][i] = 'No'\n",
    "    else:\n",
    "        data['Extranjero'][i] = 'Sí'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data.columns[[2, 3, 4, 5, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]], axis=1, inplace=True)\n",
    "data.rename(columns = {'Región (tribunal)':'Region', 'Grado desarrollo':'Desarrollo'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = data.loc[:, ~data.columns.isin(['Tipo salida 1', 'Tipo salida 2'])]\n",
    "y1 = data.loc[:, data.columns.isin(['Tipo salida 1', 'Tipo salida 2'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(x1, y1['Tipo salida 2'], test_size=0.3, random_state = 23)\n",
    "\n",
    "## Del set de entrenamiento, se desprenden 1000 datos para generar un dataset de validación\n",
    "\n",
    "x_val1 = x_train1[-1000:]\n",
    "y_val1 = y_train1[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cat_train = x_train1[['Region', 'Defensor', 'Desarrollo','Extranjero']]\n",
    "x_cat_test = x_test1[['Region', 'Defensor', 'Desarrollo','Extranjero']]\n",
    "x_cat_val = x_val1[['Region', 'Defensor', 'Desarrollo','Extranjero']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "x_cat_train['Region'] = label_encoder.fit_transform(x_cat_train['Region'])\n",
    "x_cat_train['Defensor'] = label_encoder.fit_transform(x_cat_train['Defensor'])\n",
    "x_cat_train['Desarrollo'] = label_encoder.fit_transform(x_cat_train['Desarrollo'])\n",
    "x_cat_train['Extranjero'] = label_encoder.fit_transform(x_cat_train['Extranjero'])\n",
    "\n",
    "x_cat_test['Region'] = label_encoder.fit_transform(x_cat_test['Region'])\n",
    "x_cat_test['Defensor'] = label_encoder.fit_transform(x_cat_test['Defensor'])\n",
    "x_cat_test['Desarrollo'] = label_encoder.fit_transform(x_cat_test['Desarrollo'])\n",
    "x_cat_test['Extranjero'] = label_encoder.fit_transform(x_cat_test['Extranjero'])\n",
    "\n",
    "x_cat_val['Region'] = label_encoder.fit_transform(x_cat_val['Region'])\n",
    "x_cat_val['Defensor'] = label_encoder.fit_transform(x_cat_val['Defensor'])\n",
    "x_cat_val['Desarrollo'] = label_encoder.fit_transform(x_cat_val['Desarrollo'])\n",
    "x_cat_val['Extranjero'] = label_encoder.fit_transform(x_cat_val['Extranjero'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_num_train = x_train1.loc[:, ~x_train1.columns.isin(x_cat_train.columns)]\n",
    "x_num_test = x_test1.loc[:, ~x_test1.columns.isin(x_cat_test.columns)]\n",
    "x_num_val = x_val1.loc[:, ~x_val1.columns.isin(x_cat_val.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm_train = minmax_scaler(x_num_train)\n",
    "x_norm_train = pd.DataFrame(x_norm_train, columns = x_num_train.columns)\n",
    "\n",
    "x_norm_test = minmax_scaler(x_num_test)\n",
    "x_norm_test = pd.DataFrame(x_norm_test, columns = x_num_test.columns)\n",
    "\n",
    "x_norm_val = minmax_scaler(x_num_val)\n",
    "x_norm_val = pd.DataFrame(x_norm_val, columns = x_num_val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm_train.reset_index(drop=True, inplace=True)\n",
    "x_cat_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "x_norm_test.reset_index(drop=True, inplace=True)\n",
    "x_cat_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "x_norm_val.reset_index(drop=True, inplace=True)\n",
    "x_cat_val.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_fin = pd.concat([x_norm_train, x_cat_train], axis = 1)\n",
    "x_test_fin = pd.concat([x_norm_test, x_cat_test], axis = 1)\n",
    "x_val_fin = pd.concat([x_norm_val, x_cat_val], axis = 1)\n",
    "\n",
    "y_train_fin = label_encoder.fit_transform(y_train1)\n",
    "y_test_fin = label_encoder.fit_transform(y_test1)\n",
    "y_val_fin = label_encoder.fit_transform(y_val1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Features Shape:', x_train_fin.shape)\n",
    "print('Training Labels Shape:', y_train_fin.shape)\n",
    "\n",
    "print('Testing Features Shape:', x_test_fin.shape)\n",
    "print('Testing Labels Shape:', y_test_fin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes1 = 1\n",
    "num_features1 = x_train_fin.shape[1]\n",
    "num_output1 = 1\n",
    "\n",
    "num_layers_01 = 13\n",
    "num_layers_11 = 6\n",
    "\n",
    "epochs1 = 70\n",
    "learning_rate1 = 0.01\n",
    "batch_size1 = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = keras.Input(shape = (num_features1,), name = \"data\")\n",
    "x11 = layers.Dense(num_layers_01, activation = \"sigmoid\", name = \"dense_1\")(inputs1)\n",
    "x21 = layers.Dense(num_layers_11, activation = \"sigmoid\", name = \"dense_2\")(x11)\n",
    "outputs1 = layers.Dense(num_output1, activation = \"sigmoid\", name = \"predictions\")(x21)\n",
    "\n",
    "model1 = keras.Model(inputs = inputs1, outputs = outputs1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compilando el modelo\n",
    "\n",
    "model1.compile(\n",
    "    optimizer = keras.optimizers.Adam(learning_rate = learning_rate1),\n",
    "    loss = keras.losses.BinaryCrossentropy(from_logits = False),\n",
    "    metrics = [keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.BinaryAccuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenando el modelo\n",
    "\n",
    "training1 = model1.fit(\n",
    "    x_train_fin,\n",
    "    y_train_fin,\n",
    "    batch_size = batch_size1,\n",
    "    epochs = epochs1,\n",
    "    validation_data=(x_val_fin, y_val_fin)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training1.history['binary_accuracy']\n",
    "training1.epoch\n",
    "\n",
    "plt.plot(training1.epoch, training1.history['binary_accuracy'])\n",
    "plt.title('Accuracy v/ epochs')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training1.history['precision']\n",
    "plt.plot(training1.epoch, training1.history['precision'])\n",
    "plt.title('Precision v/ epochs')\n",
    "plt.ylabel('Precision Score')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training1.history['recall']\n",
    "plt.plot(training1.epoch, training1.history['recall'])\n",
    "plt.title('Recall v/ epochs')\n",
    "plt.ylabel('Recall Score')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training1.history['loss']\n",
    "plt.plot(training1.epoch, training1.history['loss'])\n",
    "plt.title('Loss v/ epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluando el modelo\n",
    "\n",
    "results1 = model1.evaluate(x_test_fin, y_test_fin, batch_size = 1024)\n",
    "print(\"test loss, test acc, test prec, test recall:\", results1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generando predicciones\n",
    "\n",
    "predict_train1 = (model1.predict(x_train_fin) > 0.5).astype(int)\n",
    "predict_test1 = (model1.predict(x_test_fin) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Matriz de confusión para dataset de entrenamiento\n",
    "\n",
    "print(confusion_matrix(y_train_fin,predict_train1))\n",
    "print(classification_report(y_train_fin,predict_train1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Matriz de confusión para dataset de prueba\n",
    "\n",
    "print(confusion_matrix(y_test_fin,predict_test1))\n",
    "print(classification_report(y_test_fin,predict_test1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving results and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame(predict_test1, columns=['score'])\n",
    "\n",
    "x_test1_alt = x_test1.reset_index(drop=True)\n",
    "y_test1_alt = y_test1.reset_index(drop=True)\n",
    "\n",
    "test = pd.concat([x_test1_alt, test_results, pd.DataFrame(y_test_fin)], axis = 1)\n",
    "\n",
    "path2 = r'preds_dt.csv'\n",
    "test.to_csv(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save('dt_base.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model1, x_train_fin.values[:])\n",
    "shap_values = explainer(x_train_fin.values[:])\n",
    "shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values.feature_names = list(x_train_fin.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_df = pd.DataFrame(shap_values.values, columns=shap_values.feature_names)\n",
    "\n",
    "# Calcular el valor absoluto y luego el promedio para cada característica\n",
    "shap_abs_avg = shap_df.abs().mean()\n",
    "shap_avg = shap_df.mean()\n",
    "shap_max = shap_df.max()\n",
    "shap_min= shap_df.min()\n",
    "\n",
    "print('Media absoluta: ', '\\n\\n', shap_abs_avg)\n",
    "print('----------------------------------')\n",
    "print('Media: ', '\\n\\n', shap_avg)\n",
    "print('----------------------------------')\n",
    "print('Máximo: ', '\\n\\n', shap_max)\n",
    "print('----------------------------------')\n",
    "print('Mínimo: ', '\\n\\n', shap_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"dt_base_shap.pdf\", bbox_inches=\"tight\", format=\"pdf\")  # o \"shap_plot.svg\" para formato SVG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_datapoints = 5007\n",
    "tool_height_in_px = 750\n",
    "\n",
    "examples_labels = pd.concat([x_test_fin.reset_index(drop=True), pd.DataFrame(y_test_fin, columns = ['Tipo salida 2']).reset_index(drop=True)], axis=1)\n",
    "columns_not_for_model_input = [examples_labels.columns.get_loc('Tipo salida 2')]\n",
    "\n",
    "examples_wit = examples_labels.values.tolist()\n",
    "column_names = examples_labels.columns.tolist()\n",
    "\n",
    "model_inputs = np.delete(np.array(examples_wit[:num_datapoints]), columns_not_for_model_input, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the tool with the test examples and the trained classifier\n",
    "\n",
    "config_builder = WitConfigBuilder(examples_wit[:num_datapoints],column_names).set_custom_predict_fn(custom_predict).set_target_feature('Tipo salida 2')\n",
    "WitWidget(config_builder, height=tool_height_in_px)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
