{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Petty Theft experiment: base model\n",
    "\n",
    "This notebooks contains the data processing, building and training part of the model used for the base iteration of the petty theft experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import random\n",
    "import joblib\n",
    "import witwidget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from keras import layers\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from witwidget.notebook.visualization import WitWidget, WitConfigBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scaler(data):\n",
    "  scaler = MinMaxScaler()\n",
    "  scaled = scaler.fit_transform(data)\n",
    "  return scaled\n",
    "\n",
    "def process_data(data):\n",
    "  x = data.loc[:, data.columns != 'Tipo salida 2']\n",
    "  y = data['Tipo salida 2']\n",
    "\n",
    "  x_cat = x[['Region', 'Defensor', 'Desarrollo','extranjero']]\n",
    "  x_cat['Region'] = label_encoder.fit_transform(x_cat['Region'])\n",
    "  x_cat['Defensor'] = label_encoder.fit_transform(x_cat['Defensor'])\n",
    "  x_cat['Desarrollo'] = label_encoder.fit_transform(x_cat['Desarrollo'])\n",
    "  x_cat['extranjero'] = label_encoder.fit_transform(x_cat['extranjero'])\n",
    "\n",
    "  x_num = x.loc[:, ~x.columns.isin(x_cat.columns)]\n",
    "\n",
    "  x_norm = minmax_scaler(x_num)\n",
    "  x_norm = pd.DataFrame(x_norm, columns = x_num.columns)\n",
    "\n",
    "  x_norm.reset_index(drop=True, inplace=True)\n",
    "  x_cat.reset_index(drop=True, inplace=True)\n",
    "\n",
    "  x_fin = pd.concat([x_norm, x_cat], axis = 1)\n",
    "  #y_fin = label_encoder.fit_transform(y)\n",
    "\n",
    "  return x_fin\n",
    "\n",
    "def custom_predict(examples_to_infer):\n",
    "\n",
    "  preds = model.predict(model_inputs)\n",
    "  preds = [[1 - pred[0], pred[0]] for pred in preds]\n",
    "  return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'hurtoFalta.csv'\n",
    "\n",
    "data = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cambiando las dos RM a una sola\n",
    "\n",
    "data['Región (tribunal)']=data['Región (tribunal)'].replace('Metropolitana Sur','Metropolitana')\n",
    "data['Región (tribunal)']=data['Región (tribunal)'].replace('Metropolitana Norte','Metropolitana')\n",
    "\n",
    "## Agregar variable edad\n",
    "\n",
    "data['edad'] = np.nan\n",
    "\n",
    "mu = 31 ## Edad promedio entre 18 y 44 años (concentran la mayoría de los delitos de hurto)\n",
    "sigma = 8\n",
    "random.seed(23)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    data['edad'][i] = round(max(18, min(np.random.normal(mu, sigma), 65)))\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data.columns[[1, 3, 4, 5, 6, 8, 10, 11, 12]], axis=1, inplace=True)\n",
    "data.rename(columns = {'Región (tribunal)':'Region', 'Grado desarrollo':'Desarrollo'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = data.loc[:, ~data.columns.isin(['Tipo salida 1'])]\n",
    "y1 = data.loc[:, data.columns.isin(['Tipo salida 1'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(x1, y1['Tipo salida 1'], test_size=0.3, random_state = 23)\n",
    "\n",
    "## Del set de entrenamiento, se desprenden 1000 datos para generar un dataset de validación\n",
    "\n",
    "x_val1 = x_train1[-1000:]\n",
    "y_val1 = y_train1[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cat_train = x_train1[['Region', 'Defensor', 'Desarrollo']]\n",
    "x_cat_test = x_test1[['Region', 'Defensor', 'Desarrollo']]\n",
    "x_cat_val = x_val1[['Region', 'Defensor', 'Desarrollo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "x_cat_train['Defensor'] = label_encoder.fit_transform(x_cat_train['Defensor'])\n",
    "x_cat_train['Desarrollo'] = label_encoder.fit_transform(x_cat_train['Desarrollo'])\n",
    "x_cat_train['Region'] = label_encoder.fit_transform(x_cat_train['Region'])\n",
    "\n",
    "x_cat_test['Defensor'] = label_encoder.fit_transform(x_cat_test['Defensor'])\n",
    "x_cat_test['Desarrollo'] = label_encoder.fit_transform(x_cat_test['Desarrollo'])\n",
    "x_cat_test['Region'] = label_encoder.fit_transform(x_cat_test['Region'])\n",
    "\n",
    "x_cat_val['Defensor'] = label_encoder.fit_transform(x_cat_val['Defensor'])\n",
    "x_cat_val['Desarrollo'] = label_encoder.fit_transform(x_cat_val['Desarrollo'])\n",
    "x_cat_val['Region'] = label_encoder.fit_transform(x_cat_val['Region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_num_train = x_train1.loc[:, ~x_train1.columns.isin(x_cat_train.columns)]\n",
    "x_num_test = x_test1.loc[:, ~x_test1.columns.isin(x_cat_test.columns)]\n",
    "x_num_val = x_val1.loc[:, ~x_val1.columns.isin(x_cat_val.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm_train = minmax_scaler(x_num_train)\n",
    "x_norm_train = pd.DataFrame(x_norm_train, columns = x_num_train.columns)\n",
    "\n",
    "x_norm_test = minmax_scaler(x_num_test)\n",
    "x_norm_test = pd.DataFrame(x_norm_test, columns = x_num_test.columns)\n",
    "\n",
    "x_norm_val = minmax_scaler(x_num_val)\n",
    "x_norm_val = pd.DataFrame(x_norm_val, columns = x_num_val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(label_encoder.classes_)):\n",
    "  print(x, label_encoder.classes_[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cat_train['Region_alt'] = x_cat_train['Region'].apply(lambda x: 1 if x in {14, 12, 11, 8, 7, 6, 5, 4} else 0)\n",
    "x_cat_test['Region_alt'] = x_cat_test['Region'].apply(lambda x: 1 if x in {14, 12, 11, 8, 7, 6, 5, 4} else 0)\n",
    "x_cat_val['Region_alt'] = x_cat_val['Region'].apply(lambda x: 1 if x in {14, 12, 11, 8, 7, 6, 5, 4} else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_norm_train.reset_index(drop=True, inplace=True)\n",
    "x_cat_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "x_norm_test.reset_index(drop=True, inplace=True)\n",
    "x_cat_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "x_norm_val.reset_index(drop=True, inplace=True)\n",
    "x_cat_val.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_fin = pd.concat([x_norm_train, x_cat_train], axis = 1)\n",
    "x_test_fin = pd.concat([x_norm_test, x_cat_test], axis = 1)\n",
    "x_val_fin = pd.concat([x_norm_val, x_cat_val], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_fin = label_encoder.fit_transform(y_train1)\n",
    "y_test_fin = label_encoder.fit_transform(y_test1)\n",
    "y_val_fin = label_encoder.fit_transform(y_val1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Features Shape:', x_train_fin.shape)\n",
    "print('Training Labels Shape:', y_train_fin.shape)\n",
    "\n",
    "print('Testing Features Shape:', x_test_fin.shape)\n",
    "print('Testing Labels Shape:', y_test_fin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(x_train_fin, label=y_train_fin)\n",
    "val_data = lgb.Dataset(x_val_fin, label=y_val_fin)\n",
    "test_data = lgb.Dataset(x_test_fin, label=y_test_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parámetros\n",
    "\n",
    "boosting_type = 'gbdt'\n",
    "num_leaves = 63\n",
    "max_depth = -1\n",
    "learning_rate = 0.01\n",
    "n_estimators = 100\n",
    "objective = 'binary'\n",
    "#class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Diccionario de parámetros\n",
    "\n",
    "model = lgb.LGBMClassifier(\n",
    "    boosting_type = boosting_type,\n",
    "    num_leaves = num_leaves,\n",
    "    max_depth = max_depth,\n",
    "    learning_rate = learning_rate,\n",
    "    n_estimators = n_estimators,\n",
    "    objective = objective\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train_fin, y_train_fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generando predicciones\n",
    "\n",
    "predict_train1 = (model.predict_proba(x_train_fin)[0] > 0.5).astype(int)\n",
    "predict_test1 = (model.predict_proba(x_test_fin)[0] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Matriz de confusión para dataset de entrenamiento\n",
    "\n",
    "print(confusion_matrix(y_train_fin,predict_train1))\n",
    "print(classification_report(y_train_fin,predict_train1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Matriz de confusión para dataset de prueba\n",
    "\n",
    "print(confusion_matrix(y_test_fin,predict_test1))\n",
    "print(classification_report(y_test_fin,predict_test1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving results and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame(predict_test1, columns=['score'])\n",
    "\n",
    "x_test1_alt = x_test1.reset_index(drop=True)\n",
    "y_test1_alt = y_test1.reset_index(drop=True)\n",
    "\n",
    "test = pd.concat([x_test1_alt, test_results, pd.DataFrame(y_test_fin)], axis = 1)\n",
    "\n",
    "path2 = r'preds_pt_base.csv'\n",
    "test.to_csv(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, 'pt_base.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model, x_train_fin.values[:])\n",
    "shap_values = explainer(x_train_fin.values[:])\n",
    "shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values.feature_names = list(x_train_fin.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_df = pd.DataFrame(shap_values.values, columns=shap_values.feature_names)\n",
    "\n",
    "# Calcular el valor absoluto y luego el promedio para cada característica\n",
    "shap_abs_avg = shap_df.abs().mean()\n",
    "shap_avg = shap_df.mean()\n",
    "shap_max = shap_df.max()\n",
    "shap_min= shap_df.min()\n",
    "shap_median = shap_df.median()\n",
    "\n",
    "print('Media absoluta: ', '\\n\\n', shap_abs_avg)\n",
    "print('----------------------------------')\n",
    "print('Media: ', '\\n\\n', shap_avg)\n",
    "print('----------------------------------')\n",
    "print('Máximo: ', '\\n\\n', shap_max)\n",
    "print('----------------------------------')\n",
    "print('Mínimo: ', '\\n\\n', shap_min)\n",
    "print('----------------------------------')\n",
    "print('Mediana: ', '\\n\\n', shap_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"pt_base_shap.pdf\", bbox_inches=\"tight\", format=\"pdf\")  # o \"shap_plot.svg\" para formato SVG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_datapoints = y_test_fin.shape[0]\n",
    "tool_height_in_px = 750\n",
    "\n",
    "examples_labels = pd.concat([x_test_fin.reset_index(drop=True), pd.DataFrame(y_test_fin, columns = ['Tipo salida 1']).reset_index(drop=True)], axis=1)\n",
    "columns_not_for_model_input = [examples_labels.columns.get_loc('Tipo salida 1')]\n",
    "\n",
    "examples_wit = examples_labels.values.tolist()\n",
    "column_names = examples_labels.columns.tolist()\n",
    "\n",
    "model_inputs = np.delete(np.array(examples_wit[:num_datapoints]), columns_not_for_model_input, axis=1)\n",
    "\n",
    "def custom_predict_shap(examples_to_infer):\n",
    "\n",
    "  preds = model.predict_proba(model_inputs)\n",
    "  preds = [[pred[0], pred[1]] for pred in preds]\n",
    "\n",
    "  shap_output = explainer(model_inputs)\n",
    "  attributions = []\n",
    "  for single_shap_output in shap_output:  # iteramos sobre cada resultado de shap (cada ejemplo)\n",
    "    attrs = {}\n",
    "    for i, col in enumerate(x_train_fin.columns):\n",
    "      attrs[col] = single_shap_output.values[i]  # utilizamos single_shap_output.values\n",
    "    attributions.append(attrs)\n",
    "\n",
    "  ret = {'predictions': preds, 'attributions': attributions}\n",
    "\n",
    "  return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the tool with the test examples and the trained classifier\n",
    "\n",
    "config_builder = WitConfigBuilder(examples_wit[:num_datapoints],column_names).set_custom_predict_fn(custom_predict_shap).set_target_feature('Tipo salida 1')\n",
    "WitWidget(config_builder, height=tool_height_in_px)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
